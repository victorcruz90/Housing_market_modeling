{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        #Create the scraper object with some options\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        self.options.add_experimental_option('useAutomationExtension', False)\n",
    "        self.options.headless = True\n",
    "        self.driver = webdriver.Chrome(service = Service('/Users/victorcruzdefaria/Downloads/chromedriver'), options=self.options)\n",
    "        self.driver.get(url)\n",
    "\n",
    "    def get_details(self):\n",
    "\n",
    "        #access the browser\n",
    "        time.sleep(5)\n",
    "\n",
    "        #get the prices, address and date sold and add to a list\n",
    "        prices = [x.text for x in self.driver.find_elements(By.CLASS_NAME, 'css-9hd67m')]\n",
    "        full_address = [x.text for x in self.driver.find_elements(By.CLASS_NAME, 'css-bqbbuf')]\n",
    "        date_sold = [x.text for x in self.driver.find_elements(By.CLASS_NAME, 'css-1nj9ymt')]\n",
    "        housing_type = [x.text for x in self.driver.find_elements(By.CLASS_NAME, 'css-693528')]\n",
    "        \n",
    "        \n",
    "        #get the size info i.e. # of beds, # of baths, #number of parking\n",
    "        #CHALLENGE: sometimes the size of the house/unit comes after the above metrics, used regex in list comprehension to remove all the strings starting with 3 digits\n",
    "        layout_info = [x.text for x in self.driver.find_elements(By.CLASS_NAME, 'css-1ie6g1l') if not re.search(r\"[m]\", x.text)]\n",
    "        \n",
    "        #Group the layout_info into groups of 3\n",
    "        splitedSize = 3\n",
    "        layout_info = [layout_info[x:x+splitedSize] for x in range(0, len(layout_info), splitedSize)]\n",
    "        \n",
    "        #Group the data together\n",
    "        data = [[e for x in grp for e in (x if isinstance(x, list) else [x])] for grp in zip(full_address,housing_type, date_sold, layout_info, prices, )]\n",
    "        df = pd.DataFrame(data, columns=['address','housing_type', 'sold_date', 'n_beds','n_bath','n_garage', 'prices'])\n",
    "\n",
    "        #close the browser\n",
    "        self.driver.close()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prices(df):\n",
    "    df['prices'] = df['prices'].apply(lambda x: re.sub('[^0-9]+','', x)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_address(df):\n",
    "    df[['address', 'suburb']] = df['address'].str.split(',', expand=True)\n",
    "    df[['empty','suburb', 'state', 'postcode']] = df['suburb'].str.split(' ', n=3, expand=True)\n",
    "    df['suburb'] = df['suburb'].str.strip()\n",
    "    df['state'] = df['state'].astype('category')\n",
    "    df.drop(labels=['empty'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_date(df):\n",
    "    df['sold_date'] = df['sold_date'].str.split().apply(lambda x : '/'.join(x[-3:]).lower())\n",
    "    df['sold_date'] = pd.to_datetime(df['sold_date'], dayfirst=True, format=\"%d/%b/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_layout(df):\n",
    "    for i in ['n_beds', 'n_bath', 'n_garage']:\n",
    "        df[i] = df[i].str.split().apply(lambda x: x[0]).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_housing_type(df):\n",
    "    df['housing_type'] = df['housing_type'].str.split('/').apply(lambda x: x[0].strip()).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsScraper(Scraper):\n",
    "    # This class scrape the number of results available to\n",
    "    # predict how many URL will be generated\n",
    "    #TODO: LINK THE NUMBER OF RESULTS WITH PRICE RANGE\n",
    "        #This will allow us to scrape as much old data as possible\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        #Inheritance from scraper.py to create similar scraper object\n",
    "        super().__init__(url)\n",
    "\n",
    "    def get_right_pages(self):\n",
    "        #Use the Scraper\n",
    "        search_summary = self.driver.find_element(By.CLASS_NAME, 'css-ekkwk0').text\n",
    "        n_results = int(search_summary.split(' ', maxsplit=1)[0])\n",
    "\n",
    "        #Calculate the number of pages to scrape\n",
    "        #This will avoid the creation of unnecessary URL.\n",
    "        #Domain Real State only display 50 pages search\n",
    "        if n_results%20 == 0:\n",
    "            n_pages = n_results/20\n",
    "        else:\n",
    "            n_pages = (n_results//20)+1\n",
    "        print(n_results, n_pages)\n",
    "        #    return n_pages\n",
    "\n",
    "def url_generator(number_pages, state=str):\n",
    "    suburb_postcode = pd.read_csv('postcode.csv')\n",
    "    url_list = []\n",
    "    for x in suburb_postcode[state]:\n",
    "        for y in range(1,number_pages+1):\n",
    "            BASE_URL = f'https://www.domain.com.au/sold-listings/?postcode={x}&price=0-5000000&excludepricewithheld=1&page={y}'\n",
    "            url_list.append(BASE_URL)\n",
    "            print(BASE_URL)\n",
    "    print (len(url_list))        \n",
    "\n",
    "url_generator(50,'VIC')\n",
    "# n_pages = ResultsScraper('https://www.domain.com.au/sold-listings/?postcode=3168&price=0-5000000&excludepricewithheld=1')\n",
    "# n_pages.get_right_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    URL = 'https://www.domain.com.au/sold-listings/?suburb=carnegie-vic-3163,murrumbeena-vic-3163&excludepricewithheld=1'\n",
    "    web_scraper = Scraper(URL)\n",
    "    df = web_scraper.get_details()\n",
    "    cleaning_address(df)\n",
    "    cleaning_date(df)\n",
    "    cleaning_layout(df)\n",
    "    cleaning_housing_type(df)\n",
    "    clean_prices(df)\n",
    "\n",
    "    print(df.head(10))\n",
    "    df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Stock')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e090bb39366b4e31aa2c6f9a4f5a7a56bf23b9e2a11fd6f2681e7f261c753ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
